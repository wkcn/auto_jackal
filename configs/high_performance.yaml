# High-performance Training Configuration for Multi-core CPUs
# Optimized for AMD EPYC 9K84 96-Core Processor or similar high-core-count systems
# Inherits from default.yaml and only overrides specific parameters

inherit: 'default.yaml'

# ============================================================================
# Training Hyperparameters (Override)
# ============================================================================
training:
  update_interval: 2048  # Larger batch for better GPU/CPU utilization

# ============================================================================
# Checkpoint Management
# ============================================================================
checkpoint:
  save_interval: 500  # Save checkpoint every N episodes

# ============================================================================
# Visualization Settings (Override)
# ============================================================================
visualization:
  render_interval: 10
  plot_update_interval: 10

# ============================================================================
# Parallel Training Settings (Override)
# ============================================================================
parallel:
  n_workers: 32  # Utilize more cores for faster training
  show_game_screens: false  # Disable to reduce overhead
  visualization_update_interval: 1.0

# ============================================================================
# PPO Algorithm Parameters (Override)
# ============================================================================
ppo:
  batch_size: 512  # Larger batch size for better GPU utilization

# ============================================================================
# Logging Settings (Override)
# ============================================================================
logging:
  enabled: true  # âœ“ Logging enabled
  backend: 'wandb'  # Choose: 'wandb' or 'swanlab'
  tags: ['high-performance']
  notes: 'High performance training run'
  log_interval: 100  # Log less frequently for performance
  log_model: true  # Save best models
